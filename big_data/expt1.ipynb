{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d75a2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SparkSession' from 'pyspark' (/home/franz/.local/lib/python3.11/site-packages/pyspark/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfindspark\u001b[39;00m\n\u001b[32m      3\u001b[39m findspark.init()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'SparkSession' from 'pyspark' (/home/franz/.local/lib/python3.11/site-packages/pyspark/__init__.py)"
     ]
    }
   ],
   "source": [
    "!pip install pyspark findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ab461",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "        .appName(\"Expt1\")\\\n",
    "            .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca9df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_text = textfile('blah.txt ')\n",
    "in_text = collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bd5608",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_flatten = in_text.flatMap(lambda x: x.split(\"\"))\\\n",
    "    .map(lambda x: (x,1))\n",
    "    .reducebyKey(lambda x,y : x+y)\n",
    "text_file.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4b1364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.5.0\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting findspark\n",
      "  Using cached findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Collecting py4j==0.10.9.7 (from pyspark==3.5.0)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml): started\n",
      "  Building wheel for pyspark (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425400 sha256=f27036dfe1f636946bc2ccf56c64c44e228cd45e96bde5086060fa6a89d9fdc0\n",
      "  Stored in directory: /home/franz/.cache/pip/wheels/84/40/20/65eefe766118e0a8f8e385cc3ed6e9eb7241c7e51cfc04c51a\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, findspark, pyspark\n",
      "Successfully installed findspark-2.0.1 py4j-0.10.9.7 pyspark-3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Run this in a notebook cell to create and activate venv\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Create virtual environment if it doesn't exist\n",
    "venv_path = \"/home/franz/venvs/spark_env\"\n",
    "if not os.path.exists(venv_path):\n",
    "    subprocess.run([sys.executable, \"-m\", \"venv\", venv_path])\n",
    "\n",
    "# Install packages in virtual environment\n",
    "subprocess.run([f\"{venv_path}/bin/pip\", \"install\", \"pyspark==3.5.0\", \"findspark\"])\n",
    "\n",
    "# Add venv to Python path\n",
    "sys.path.insert(0, f\"{venv_path}/lib/python3.11/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62acf0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n",
      "File content length: 0\n",
      "First 100 characters: \n"
     ]
    }
   ],
   "source": [
    "# Add this cell to check your file\n",
    "import os\n",
    "\n",
    "file_path = \"/home/franz/Documents/Lab/big_data/blah.txt\"\n",
    "print(f\"File exists: {os.path.exists(file_path)}\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        print(f\"File content length: {len(content)}\")\n",
    "        print(f\"First 100 characters: {content[:100]}\")\n",
    "else:\n",
    "    print(\"File not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f696d6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue: 5\n",
      "sky: 2\n",
      "hello: 1\n",
      "bonjor: 1\n",
      "it: 1\n",
      "world: 1\n",
      "is: 1\n",
      "a: 1\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup (run after activating virtual environment)\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Cell 2: Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"WordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Cell 3: Word count processing\n",
    "text_rdd = sc.textFile(\"/home/franz/Documents/Lab/big_data/blah.txt\")\n",
    "words = text_rdd.flatMap(lambda line: line.split())\n",
    "word_counts = words.map(lambda word: (word, 1)) \\\n",
    "                  .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "sorted_counts = word_counts.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Print results\n",
    "results = sorted_counts.collect()\n",
    "for word, count in results:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Cell 4: Cleanup\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
