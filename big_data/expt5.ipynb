{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ff364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/08 14:38:42 WARN Utils: Your hostname, kingstein, resolves to a loopback address: 127.0.1.1; using 10.122.78.224 instead (on interface wlp3s0)\n",
      "25/09/08 14:38:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/08 14:38:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c962956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created from collection:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|ALice| 22|\n",
      "|  Bob| 22|\n",
      "|Cathy| 23|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"ALice\",22),(\"Bob\",22),(\"Cathy\",23)]\n",
    "columns = [\"Name\",\"Age\"]\n",
    "df1 = spark.createDataFrame(data,columns)\n",
    "print(\"DataFrame created from collection:\")\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abebf95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created from CSV file:\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 22|\n",
      "|  Bob| 22|\n",
      "|Cathy| 23|\n",
      "|David| 24|\n",
      "|  Eve| 25|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.csv(\"sample.csv\",header = True, inferSchema = True)\n",
    "print(\"DataFrame created from CSV file:\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65bc3397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting only Name Column:\n",
      "+-----+\n",
      "| Name|\n",
      "+-----+\n",
      "|ALice|\n",
      "|  Bob|\n",
      "|Cathy|\n",
      "+-----+\n",
      "\n",
      "Filtering rows where Age > 23:\n",
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Selecting only Name Column:\")\n",
    "df1.select(\"Name\").show()\n",
    "print(\"Filtering rows where Age > 23:\")\n",
    "df1.filter(df1.Age > 23).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5da1d7f",
   "metadata": {},
   "source": [
    "## Add new Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3e488de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Adding a new Rows:\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Frank| 20|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data = [(\"Frank\",20)]\n",
    "new_df = spark.createDataFrame(new_data,columns)\n",
    "print(\"After Adding a new Rows:\")\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4318b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Deleting rows where Age = 22:\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Cathy| 23|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_deleted = df1.filter(df1.Age != 22)\n",
    "print(\"After Deleting rows where Age = 22:\")\n",
    "df_deleted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8615378b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After increasing Age by one\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|ALice| 23|\n",
      "|  Bob| 23|\n",
      "|Cathy| 24|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df1_updated = df1.withColumn(\"Age\",col(\"Age\") + 1)\n",
    "print(\"After increasing Age by one\")\n",
    "df1_updated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f9b3afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding a new column 'City':\n",
      "+-----+---+-----+\n",
      "| Name|Age| City|\n",
      "+-----+---+-----+\n",
      "|ALice| 22|ALice|\n",
      "|  Bob| 22|  Bob|\n",
      "|Cathy| 23|Cathy|\n",
      "+-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_with_city = df1.withColumn(\"City\", col(\"Name\"))\n",
    "print(\"After adding a new column 'City':\")\n",
    "df1_with_city.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f5af296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Dropping the 'City' column:\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|ALice| 22|\n",
      "|  Bob| 22|\n",
      "|Cathy| 23|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_dropped = df1_with_city.drop(\"City\")\n",
    "print(\"After Dropping the 'City' column:\")\n",
    "df1_dropped.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
